{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOlGrRF/n7g2eUzLHDkSjU1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7cEariCY6f1","executionInfo":{"status":"ok","timestamp":1724412061774,"user_tz":300,"elapsed":1025,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"23e537fb-f3b2-440d-de02-9f1153f000e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Estados : [0, 1, 2, 3]\n","Acciones : [0, 1]\n","Recompensas : {0: -1, 1: -1, 2: -1, 3: 10}\n"]}],"source":["# Ejercicio 1: Introducción a los principales algoritmos de RL\n","# Definición del entorno de juego\n","\n","class Environment:\n","    def __init__(self):\n","      self.state = {}\n","      self.state['space'] = [0, 1, 2, 3]\n","      self.action_space = [0, 1] # Acciones posibles\n","      self.reward = {0: -1, 1: -1, 2: -1, 3: 10} # Recompensas por estado\n","\n","# Instancia del entorno\n","env = Environment()\n","\n","# Muestra de la información del entorno\n","print(\"Estados :\", env.state['space'])\n","print(\"Acciones :\", env.action_space)\n","print(\"Recompensas :\", env.reward)"]},{"cell_type":"markdown","source":["Q-Learning"],"metadata":{"id":"61f41KE8dSyO"}},{"cell_type":"code","source":["from os import stat\n","# Ejercicio 2: Q-Learning\n","import numpy as np\n","\n","# Inicialización de la tabla Q con valores arbitrarios\n","Q = np.zeros((len(env.state['space']), len(env.action_space)))\n","\n","# Hiperparámetros del algoritmo\n","alpha = 0.1  # Tasa de aprendizaje\n","gamma = 0.9  # Factor de descuento\n","\n","num_episodes = 1000  # Número de episodios de entrenamiento\n","\n","# Entrenamiento del agente\n","for _ in range(num_episodes):\n","  state = np.random.choice(env.state['space'])  # Estado inicial aleatorio\n","  while state != 3:  # Hasta llegar al estado objetivo\n","    action = np.random.choice(env.action_space)  # Acción aleatoria\n","    next_state = state + action  # Siguiente estado\n","    reward = env.reward[next_state]  # Recompensa del siguiente estado\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma *\n","                                                   np.max(Q[next_state]) -\n","                                                   Q[state, action])\n","    state = next_state\n","\n","# Muestreo de la función Q-valor aprendida\n","print(\"Tabla Q-valor aprendida:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRqW-t19dNfh","executionInfo":{"status":"ok","timestamp":1724412061775,"user_tz":300,"elapsed":68,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"6ac901f0-2b4a-414b-ca5a-1e47c8ad868a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida:\n","[[ 4.58  6.2 ]\n"," [ 6.2   8.  ]\n"," [ 8.   10.  ]\n"," [ 0.    0.  ]]\n"]}]},{"cell_type":"markdown","source":["Sarsa"],"metadata":{"id":"JWI6HY7OgPxN"}},{"cell_type":"code","source":["# Ejercicio 3\n","# Reinicializar la tabla Q\n","Q = np.zeros((len(env.state['space']), len(env.action_space)))\n","\n","# Entrenamiento del agente con Sarsa\n","for _ in range(num_episodes):\n","  state = np.random.choice(env.state['space'])  # Estado inicial aleatorio\n","  action = np.random.choice(env.action_space)  # Acción inicial aleatoria\n","  while state != 3: # Hasta llegar al objetivo\n","    next_state = state + action  # Siguiente estado\n","    reward = env.reward[next_state]  # Recompensa del siguiente estado\n","    next_action = np.random.choice(env.action_space)  # Acción siguiente aleatoria\n","    Q[state, action] = Q[state, action] + alpha * (reward + gamma *\n","                                                   Q[next_state, next_action] -\n","                                                   Q[state, action])\n","    state = next_state\n","    action = next_action\n","\n","# Muestreo de la función Q-valor aprendida\n","print(\"Tabla Q-valor aprendida con Sarsa:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gsTFKdugRCj","executionInfo":{"status":"ok","timestamp":1724412061781,"user_tz":300,"elapsed":62,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"a642a97c-eea2-49a6-ea29-de1883fd918e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Tabla Q-valor aprendida con Sarsa:\n","[[ 1.18158616  3.47885741]\n"," [ 3.16186349  5.50391387]\n"," [ 5.69965381 10.        ]\n"," [ 0.          0.        ]]\n"]}]},{"cell_type":"markdown","source":["Política del gradiente de Montecarlo"],"metadata":{"id":"rCpjeijchO_o"}},{"cell_type":"code","source":["# Ejercicio 4\n","# Inicialización de la política con probabilidades uniformes\n","\n","policy = np.ones((len(env.state['space']), len(env.action_space))) / len(env.action_space)\n","len(env.action_space)\n","\n","# Función de recompensa promedio\n","def average_reward(Q):\n","  return np.mean([Q[state, np.argmax(policy[state])] for state in env.state['space']])\n","\n","# Entrenamiento de la política usando gradiente de Montecarlo\n","for _ in range(num_episodes):\n","  state = np.random.choice(env.state['space']) # Estado inicial aleatorio\n","  while state != 3: # Hasta llegar al objetivo\n","    action = np.random.choice(env.action_space, p=policy[state]) # Acción según la política\n","    next_state = state + action # Siguiente estado\n","    reward = env.reward[next_state] # Recompensa del siguiente estado\n","    gradient = np.zeros_like(policy[state])\n","    gradient[action] = 1\n","    alpha = 0.01\n","    policy[state] += alpha * gradient * (reward - average_reward(Q))\n","\n","    #Normalizar probabilidades para llegar a 1 y N° positivos\n","    policy[state] = np.maximum(policy[state], 0)\n","    policy[state] /= np.sum(policy[state])\n","\n","    state = next_state\n","\n","# Muestreo de la política aprendida\n","print(\"Política aprendida:\")\n","print(policy)"],"metadata":{"id":"iOx_T8CNhd2f","executionInfo":{"status":"ok","timestamp":1724412062382,"user_tz":300,"elapsed":650,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"57a1a402-91e1-4f75-8512-37a12cc66afb","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Política aprendida:\n","[[0.  1. ]\n"," [0.  1. ]\n"," [0.  1. ]\n"," [0.5 0.5]]\n"]}]}]}