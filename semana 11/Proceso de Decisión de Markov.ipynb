{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNNxIBxSsiXRjsxqUnPoEE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpA3JwdqMFL9","executionInfo":{"status":"ok","timestamp":1724414768134,"user_tz":300,"elapsed":21,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"01f3a8f9-8a2d-4212-ed01-44365b0b4ba6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Estado actual:  B\n","Acción tomada:  arriba\n","Nuevo estado:  A\n","Recompensa:  9\n"]}],"source":["import numpy as np\n","import random\n","\n","#Definición de estados, acciones y recompensas\n","estados = ['A', 'B', 'C']\n","acciones = ['arriba', 'abajo']\n","recompensas = np.random.randint(0, 10, size=(len(estados), len(acciones)))\n","\n","# Ejemplo de clase MDP con lógica de transición de ejemplo hecha en chatGPT\n","class MDP:\n","    def __init__(self, estados, acciones, recompensas):\n","        self.estados = estados\n","        self.acciones = acciones\n","        self.recompensas = recompensas\n","        # Lógica de transición de ejemplo (aleatoria)\n","        self.transiciones = {}\n","        for estado in estados:\n","            self.transiciones[estado] = {}\n","            for accion in acciones:\n","                self.transiciones[estado][accion] = {}\n","                for nuevo_estado in estados:\n","                    # Probabilidad aleatoria de transición (puedes ajustar esto)\n","                    self.transiciones[estado][accion][nuevo_estado] = random.random()\n","                # Normalizar las probabilidades para que sumen 1\n","                total = sum(self.transiciones[estado][accion].values())\n","                for nuevo_estado in estados:\n","                    self.transiciones[estado][accion][nuevo_estado] /= total\n","\n","\n","# Función de transición aleatoria\n","def transicion_aleatoria():\n","    return random.choice(estados)\n","\n","# Generación de datos\n","estado_actual = random.choice(estados)\n","accion = random.choice(acciones)\n","nuevo_estado = transicion_aleatoria()\n","recompensa = recompensas[estados.index(estado_actual), acciones.index(accion)]\n","\n","print(\"Estado actual: \", estado_actual)\n","print(\"Acción tomada: \", accion)\n","print(\"Nuevo estado: \", nuevo_estado)\n","print(\"Recompensa: \", recompensa)\n"]},{"cell_type":"markdown","source":["Conceptos de MDP"],"metadata":{"id":"2mO2maoBNaew"}},{"cell_type":"code","source":["def calcular_valor_estado(mdp, gamma=0.9, theta=0.01):\n","    valores = {estado: 0 for estado in mdp.estados}\n","    while True:\n","        delta = 0\n","        for estado in mdp.estados:\n","            valor_previo = valores[estado]\n","            # Iteración entre acciones y nuevos estados, tomando los índices para 'recompensas'\n","            valores[estado] = sum(\n","                mdp.transiciones[estado][accion][nuevo_estado] *\n","                (mdp.recompensas[mdp.estados.index(estado), mdp.acciones.index(accion)] + # 'recompensas'\n","                 gamma * valores[nuevo_estado])\n","                for accion in mdp.acciones for nuevo_estado in mdp.estados\n","            )\n","            delta = max(delta, abs(valor_previo - valores[estado]))\n","        if delta < theta:\n","            break\n","    return valores\n","\n","# Crea una instancia de la clase MDP\n","mdp = MDP(estados, acciones, recompensas)\n","\n","# Ejemplo de uso\n","valores_estados = calcular_valor_estado(mdp)\n","print(\"valores de los estados\", valores_estados)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"im6oMhs_H6P7","executionInfo":{"status":"ok","timestamp":1724414768541,"user_tz":300,"elapsed":73,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"046832fb-162a-488c-a743-d006abfdc8f1"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["valores de los estados {'A': inf, 'B': inf, 'C': inf}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-15-e49d3f165e04>:8: RuntimeWarning: overflow encountered in scalar add\n","  valores[estado] = sum(\n","<ipython-input-15-e49d3f165e04>:14: RuntimeWarning: invalid value encountered in scalar subtract\n","  delta = max(delta, abs(valor_previo - valores[estado]))\n"]}]},{"cell_type":"markdown","source":["Propiedades de Markov"],"metadata":{"id":"L-LAr6q9U4Xe"}},{"cell_type":"code","source":["def verificar_propiedades_markov(mdp):\n","  for estado in mdp.estados:\n","    for accion in mdp.acciones:\n","      suma_prob = sum(mdp.transiciones[estado][accion].values())\n","      if not np.isclose(suma_prob, 1.0):\n","        print(f\"La suma de las probabilidades de transición para el estado {estado} y la acción {accion} no es igual a 1.0\")\n","        return False\n","  return True\n","\n","# Ejemplo de uso\n","if verificar_propiedades_markov(mdp):\n","  print(\"Las propiedades de Markov se cumplen.\")\n","else:\n","  print(\"Las propiedades de Markov no se cumplen.\")"],"metadata":{"id":"jUpV2UktU6p1","executionInfo":{"status":"ok","timestamp":1724414768542,"user_tz":300,"elapsed":68,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e07887d4-7fa5-4559-994c-d0631586f759"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Las propiedades de Markov se cumplen.\n"]}]},{"cell_type":"markdown","source":["Propiedad de recompensa"],"metadata":{"id":"AyOAprdtWrwI"}},{"source":["def calcular_recompensa_promedio(mdp):\n","  recompensa_total = 0\n","  acciones_total = 0\n","  for i, estado in enumerate(mdp.estados): # Toma de índice y estado\n","    for j, accion in enumerate(mdp.acciones): # Toma de índice y acción\n","      for k, nuevo_estado in enumerate(mdp.estados): # Toma de índice y nuevo estado\n","        recompensa_total += mdp.recompensas[i, j] # Uso de índices para las recompensas\n","        acciones_total += 1\n","  return recompensa_total / acciones_total\n","\n","# Ejemplo de uso\n","recompensa_promedio = calcular_recompensa_promedio(mdp)\n","print(\"Recompensa promedio:\", recompensa_promedio)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PpDQRAPGJ2fV","executionInfo":{"status":"ok","timestamp":1724415177554,"user_tz":300,"elapsed":390,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"ca48a7f1-d474-443b-bbc9-ece8db7aab3c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Recompensa promedio: 4.0\n"]}]}]}