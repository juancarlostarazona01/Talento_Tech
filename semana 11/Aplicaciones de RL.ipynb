{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPF9EqsL0ig/YRc6K4nrwcc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Ejercicio 1: Implementación de\n","\n","Q-Learning en un entorno de gridworld simple"],"metadata":{"id":"Hpo1wXbg65Nl"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"kqcBruqh6ijC","executionInfo":{"status":"ok","timestamp":1724412851021,"user_tz":300,"elapsed":109,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}}},"outputs":[],"source":["import numpy as np\n","\n","# Definición del gridworld\n","gridworld = np.array([\n","    [-1, -1, -1,  1],\n","    [-1, -1, -1, -1],\n","    [-1, -1, -1, -1],\n","    [-1, -1, -1, -1]\n","])"]},{"cell_type":"code","source":["# Definición de ñas posibles acciones: Arriba, abajo, izquierda, derecha\n","\n","acciones = {\n","    'arriba': (0, -1),\n","    'abajo': (0, 1),\n","    'izquierda': (-1, 0),\n","    'derecha': (1, 0)\n","}"],"metadata":{"id":"CAY5fYie7lJj","executionInfo":{"status":"ok","timestamp":1724412851023,"user_tz":300,"elapsed":103,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Implementación de Q-Learning\n","Q = np.zeros_like(gridworld)\n","\n","gamma = 0.8\n","alpha = 0.1\n","num_episodes = 1000"],"metadata":{"id":"LQsJa2r79SVb","executionInfo":{"status":"ok","timestamp":1724412851025,"user_tz":300,"elapsed":103,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["for _ in range(num_episodes):\n","    estado = (0, 0)\n","    while estado != (0, 3):\n","        accion = np.random.choice(range(len(acciones)))\n","        nueva_fila = estado[0] + acciones[accion][0]\n","        nueva_col = estado[1] + acciones[accion][1]\n","        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_col < gridworld.shape[1]:\n","            recompensa = gridworld[nueva_fila, nueva_col]\n","            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n","            Q[estado][accion] = (1 - alpha) * Q[estado][accion] + alpha * nuevo_valor\n","            estado = (nueva_fila, nueva_col)\n","\n","print(\"Q-values después del entrenamiento:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"q1X4ok_g-EDm","executionInfo":{"status":"error","timestamp":1724415337225,"user_tz":300,"elapsed":631,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"18ceb987-a91a-4446-f8ec-c4260f8470c2"},"execution_count":19,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"unsupported operand type(s) for +: 'int' and 'str'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-e626ff57655d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mestado\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0maccion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macciones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnueva_fila\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macciones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnueva_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macciones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnueva_fila\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnueva_col\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mgridworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"]}]},{"cell_type":"markdown","source":["Ejercicio 2: Aplicación del Aprendizaje\n","\n","por Refuerzo en juegos"],"metadata":{"id":"_604-HUpBbOI"}},{"cell_type":"code","source":["# Ejercicio 2: Aplicación del aprendizaje por Refuerzo en juegos\n","# Ejemplo: Implementación de Q-Learning para un juego simple\n","import numpy as np\n","\n","# Definición de las recompensas del juego (datos ficticios)\n","recompensas = {\n","    'ganar' : 1,\n","    'perder' : -1,\n","    'continuar' : 0\n","}"],"metadata":{"id":"MNdtdeD7Bb7W","executionInfo":{"status":"ok","timestamp":1724413386743,"user_tz":300,"elapsed":480,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Implementación de Q.Learning para el juego\n","Q = {}\n","\n","def q_learning_juego(estado, accion, nuevo_estado, resultado):\n","    if estado not in Q:\n","        Q[estado] = np.zeros(len(acciones))\n","    if nuevo_estado not in Q:\n","        Q[nuevo_estado] = np.zeros(len(acciones))\n","        nuevo_valor = recompensa + gamma * np.max(Q[nuevo_estado])\n","        Q[estado][accion] = (1 - alpha) * Q[estado][accion] + alpha * nuevo_valor\n","\n","'''Uso de la función q_learning_juego para actualizar los valores Q,\n","se asume que se ha realizado una partida y se tiene información sobre\n","los estados, acciones, resultados, etc.\n","\n","Se llama a la función q_learning_juego con los parametros adecuados'''\n","#q_learning_juego(estado, accion, nuevo_estado, resultado)\n","\n","print(\"Q-values después del entrenamiento:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30QEOTR2CFz1","executionInfo":{"status":"ok","timestamp":1724413386746,"user_tz":300,"elapsed":47,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"3b9aa77a-ad46-44c6-b1f9-471f243d714e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-values después del entrenamiento:\n","{}\n"]}]},{"cell_type":"markdown","source":["Ejercicio 3: Aplicación del Aprendizaje\n","\n","por Refuerzo en robótica"],"metadata":{"id":"DpRdpO2kDuDY"}},{"cell_type":"code","source":["# Definición del entorno de navegación (datos ficticios)\n","\n","entorno = np.array([\n","    [0,  0,  0,  0, 0],\n","    [0, -1, -1, -1, 0],\n","    [0,  0, -1,  0, 0],\n","    [0, -1, -1, -1, 0],\n","    [0,  0,  0,  0, 0]\n","])\n","\n","# Definición de acciones posibles (arriba, abajo, izquierda, derecha)\n","acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]"],"metadata":{"id":"6Dq8cZLHDhQD","executionInfo":{"status":"ok","timestamp":1724413386747,"user_tz":300,"elapsed":34,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Implementación del Q_Learning\n","Q = np.zeros((entorno.shape[0], entorno.shape[1], len(acciones)))\n","\n","gamma = 0.9 # Descuento\n","alpha = 0.1 # Aprendizaje\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","    estado = (0, 0)\n","    while True:\n","        accion = np.random.choice(range(len(acciones)))\n","        nueva_fila = estado[0] + acciones[accion][0]\n","        nueva_col = estado[1] + acciones[accion][1]\n","        if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_col < entorno.shape[1]:\n","            recompensa = entorno[nueva_fila, nueva_col]\n","            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n","            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n","            estado = (nueva_fila, nueva_col)\n","            if recompensa == -1:\n","                break\n","        else:\n","            break\n","\n","print(\"Q-values después del entrenamiento:\")\n","print(Q)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dO2cnk6xEk5x","executionInfo":{"status":"ok","timestamp":1724413387061,"user_tz":300,"elapsed":345,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"78897847-0d22-4361-8ae0-35fb8f2354ff"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Q-values después del entrenamiento:\n","[[[ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.         -0.99970031]\n","  [ 0.          0.          0.         -0.6861894 ]\n","  [ 0.          0.          0.         -0.6861894 ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -0.99990595  0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.19        0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.         -0.56953279 -0.19       -0.271     ]\n","  [ 0.          0.          0.          0.        ]\n","  [-0.1         0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.         -0.3439      0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]\n","\n"," [[ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]\n","  [ 0.          0.          0.          0.        ]]]\n"]}]},{"cell_type":"markdown","source":["Ejercicio 4: Aplicación del Aprendizaje\n","por Refuerzo en gestión de recursos"],"metadata":{"id":"3gQKKsOHF8tB"}},{"cell_type":"code","source":["import numpy as np\n","# Definición de los estados (niveles de inventario), acciones\n","# (órdenes de reabastecimiento) y recompensas (costos, ganancias, etc.)\n","\n","estados = ['bajo', 'medio', 'alto']\n","acciones = ['Reabastecer', 'No reabastecer']\n","recompensas = {\n","    ('bajo', 'Reabastecer'): 50,\n","    ('bajo', 'No reabastecer'): -10,\n","    ('medio', 'Reabastecer'): 30,\n","    ('medio', 'No reabastecer'): 0,\n","    ('alto', 'Reabastecer'): 10,\n","    ('alto', 'No reabastecer'): -20\n","}"],"metadata":{"id":"QIem1CSpF491","executionInfo":{"status":"ok","timestamp":1724413387063,"user_tz":300,"elapsed":35,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Implementación del Q-Learning\n","Q = {}\n","\n","gamma = 0.9\n","alpha = 0.1\n","num_episodes = 1000\n","\n","for _ in range(num_episodes):\n","    estado = np.random.choice(estados)\n","    while True:\n","        accion = np.random.choice(acciones)\n","        recompensa = recompensas[estado, accion]\n","        if estado not in Q:\n","            Q[estado] = {}\n","        if accion not in Q[estado]:\n","            Q[estado][accion] = 0\n","            nuevo_estado = np.random.choice(estados)\n","            max_nuevo_estado = max(Q[nuevo_estado].values())\n","        if nuevo_estado in Q else 0\n","            Q[estado][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado][accion])\n","            estado = nuevo_estado\n","        if recompensa == 50 or recompensa == 30 or recompensa == 10:\n","            break\n","\n","print(\"Q-values después del entrenamiento:\")\n","print(Q)"],"metadata":{"id":"gtU28ZRiGkab","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1724415429479,"user_tz":300,"elapsed":355,"user":{"displayName":"Juan Tarazona","userId":"14842905058619609943"}},"outputId":"37afe24a-526b-4813-e03c-d1c29e768492"},"execution_count":20,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-20-00e3665f6e18>, line 19)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-00e3665f6e18>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    if nuevo_estado in Q else 0\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}]}